<!DOCTYPE html><!--[if lt IE 8]><html lang="en" class="ie"><![endif]--><!--[if gt IE 8]><!--><html lang="en"><!--<![endif]--><head><meta charset="utf-8"><title>MOLGENIS - MOLGENIS</title><meta name="description"><meta name="author" content="MOLGENIS community"><meta name="handheldfriendly" content="true"><meta name="mobileoptimized" content="320"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta http-equiv="cleartype" content="on"><script src="/res/js/jquery-1.11.3.min.js"> </script><script src="/res/js/bootstrap.min.js"></script><link rel="stylesheet" href="/res/css/prism.css"><link rel="stylesheet" href="/res/css/application.css" type="text/css"><link rel="shortcut icon" href="/favicon.png" type="image/png"><link rel="alternate" type="application/rss+xml" href="/blog/feed.xml" title="undefined"></head><body><header class="molgenis-docs-header"><div role="navigation" class="navbar navbar-fixed-top"><div class="header-container"><div class="navbar-header"><button type="button" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar" class="navbar-toggle collapsed"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a href="/" class="navbar-brand active"><img src="/res/images/wordmark-blue.png" alt="undefined" class="wordmark"/></a></div><div class="collapse navbar-collapse">              <ul class="nav navbar-nav"><li><a href="/software">TOOLS</a></li><li><a href="/documentation">DOCUMENTATION</a></li><li class="active"><a href="/pipelines">PIPELINES</a></li><li><a href="/develop">DEVELOP</a></li><li><a href="/support">SUPPORT</a></li><li><a href="#searchbox" data-toggle="collapse"><i class="glyphicon glyphicon-search"></i></a></li></ul></div></div></div><div id="searchbox" class="collapse"><form role="search" action="/search-results"><input placeholder="Search" name="q" class="form-control"/></form></div><div class="molgenis-docs-titlebar active"><div class="container"> <h1>MOLGENIS</h1><p>   </p></div></div></header><div class="molgenis-docs-body"><div class="container"><div class="row"><div role="complementary" class="col-md-3"><nav id="molgenis-docs-sidebar" class="molgenis-docs-sidebar hidden-print hidden-xs hidden-sm"><ul id="molgenis-docs-sidenav" class="nav molgenis-docs-sidenav"></ul></nav><script>$(function() {
    var sideNav = $("#molgenis-docs-sidenav");
    var index = 0;
    $(".row h1").each(function() {
        
        //create id for h1
        var id = (index++) + $(this).text().replace(/\s/gi,"-").replace(/\W/g,"").toLowerCase();
        $(this).prepend("<a name=\""+id+"\"></a>");
        
        //add h1 to list
        var li = $("<li><a href=\"#"+id+"\">"+$(this).text()+"</a></li>")
        sideNav.append(li);
        
        //wrap h1 into section
        var section = $(this).wrap("<section id=\""+id+"\"></section>").parent();
        
        //wrap parts of h1 into one section
        $(this).parent().nextUntil("h1").each(function(){            
            section.append($(this));
        });
        
        //repeat for h2
        if($("h2",$(this).parent()).length > 0) {
            var ul = $("<ul class=\"nav\"></ul>");
            li.append(ul);
            
            $("h2",$(this).parent()).each(function(){
            
                //create id for h2
                var id = (index++) + $(this).text().replace(/\s/gi,"-").replace(/\W/g,"").toLowerCase();
                $(this).prepend("<a name=\""+id+"\"></a>");
        
                //wrap h2 into section
                var section = $(this).wrap("<section id=\""+id+"\"></section>").parent();
        
                ul.append($("<li><a href=\"#"+id+"\">"+$(this).text()+"</a></li>"));
        
                //wrap parts of h2 into one section
                $(this).parent().nextUntil("h2").each(function(){            
                    section.append($(this));
                });
            

            });
        }
    });    
    
    //add "back to top"
    sideNav.append("<li><a href=\"#top\" style=\"font-size:smaller\">Back to top</a></li>");
    var $body   = $(document.body);
    
    //enable the 'active' highlighting using scrollspy plugin
    $('body').scrollspy({
        target: '#molgenis-docs-sidebar'
    });
    $('#molgenis-docs-sidebar').width($('#molgenis-docs-sidebar').parent().width());
    
    //enable affix
    $('#molgenis-docs-sidebar').affix({
        offset: {
            top: $('#molgenis-docs-sidebar').offset().top,
            bottom: $('.molgenis-docs-footer').outerHeight(true) + 10
        }
    });
    
    //resize affix box when window resizes
    $(window).resize(function () {
            $('.molgenis-docs-sidebar').width($('#molgenis-docs-sidebar').parent().width());
        });
    
    $("table").addClass("table table-bordered");
});</script></div><div role="main" class="col-md-9 contents"><h1>Introduction</h1><p>Made for bioinformatics and life sciences, MOLGENIS compute is a flexible shell script 
framework to generate big data workflows that can run parallel on clusters and grids. 
Molgenis Compute Users can:</p>
<ul>
<li>Design a workflow.csv with each step as a shell script &#39;protocol&#39;;</li>
<li>Generate and run jobs by iterating over parameters.csv and execute on a compute backend;</li>
<li>(Optional) use standardized file and tool management for portable workflows.</li>
</ul>
<h2>Get Started</h2><p>The latest distribution of Molgenis Compute is available 
at the <a href="http://www.molgenis.org/wiki/ComputeStart">Molgenis Compute start page</a>. 
Unzip the archive and the command-line version is ready to use.  </p>
<pre><code>  unzip molgenis-compute-core-0.0.1-SNAPSHOT-distribution.zip
  cd molgenis-compute-core-0.0.1-SNAPSHOT
</code></pre><h2>Create workflow</h2><p>Now u can create your first workflow by executing the following command</p>
<pre><code>  sh molgenis_compute.sh --create myfirst_workflow
</code></pre><p>Afterwards go to the created workflow directory.</p>
<pre><code>  cd myfirst_workflow
</code></pre><p>You see the typical Molgenis Compute workflow structure</p>
<pre><code>  /protocols              #folder with bash script &#39;protocols&#39;
  /protocols/step1.sh     #example of a protocol shell script
  /protocols/step2.sh     #example of a protocol shell script
  workflow.csv            #file listing steps and parameter flow
  workflow.defaults.csv   #default parameters for workflow.csv (optional)
  parameters.csv          #parameters you want to run analysis on
  header.ftl              #user extra script header (optinal)
  footer.ftl              #user extra script footer (optinal)
</code></pre><p>A similar structure should be created for every workflow. Additionally, a workflow can be read from the github repository, if the github root is specified.</p>
<p>In the easiest scenario, the <code>workflow.csv</code> file has the following structure:</p>
<pre><code>  step,protocol,dependencies
  step1,protocols/step1.sh,
  step2,protocols/step2.sh,step1
</code></pre><p>This means that the workflow consists of two steps &#39;step1&#39; and &#39;step2&#39;, where &#39;step2&#39; depends on &#39;step1&#39;. &#39;step1&#39; has its analysis protocol in the file protocols/step1.sh and &#39;step2&#39; in the file protocols/step2.sh respectively.</p>
<p>The created workflow in our example <code>workflow.csv</code> is a bit more complex. Let&#39;s first look at the <code>parameters.csv</code> file, which contains some workflow parameters. In this example, one parameter &#39;input&#39; has two values &#39;hello&#39; and &#39;bye&#39;:</p>
<pre><code>  input
  hello
  bye
</code></pre><p>These parameters can be used in protocols.
In the following protocol example</p>
<pre><code>  #string input
  #output out
  # Let&#39;s do something with string &#39;input&#39;
  echo ${input}_hasBeenInStep1
  out=${input}_hasBeenInStep1
</code></pre><p>&#39;input&#39; will be substituted with values &#39;hello&#39; or &#39;bye&#39;. In the header of protocols, 
we specify inputs with flags &#39;#string&#39; for variables with a single value 
and &#39;#list&#39; for variables with multiple values. The outputs are specified with the flag &#39;#output&#39;</p>
<p>In our example protocol step1.sh, we would like to call &#39;input&#39; as &#39;in&#39;:</p>
<pre><code>  #string in
  #output out
  # Let&#39;s do something with string &#39;in&#39;
  echo ${in}_hasBeenInStep1
  out=${in}_hasBeenInStep1
</code></pre><p>In this case, we need to map these names in our example workflow.csv file</p>
<pre><code>  step,protocol,parameterMapping
  step1,protocols/step1.sh,in=input
  step2,protocols/step2.sh,wf=workflowName;date=creationDate;strings=step1.out

  in=input
</code></pre><p>This does the trick. In the same way, we can map outputs of one step to the inputs of the next steps. In our example, &#39;strings&#39; in the &#39;step2&#39;, 
which has protocol step2.ftl</p>
<pre><code>  #string wf
  #string date
  #list strings
  echo &quot;Workflow name: ${wf}&quot;
  echo &quot;Created: ${date}&quot;
  echo &quot;Result of step1.sh:&quot;
  for s in &quot;${strings[@]}&quot;
  do
    echo ${s}
  done
  echo &quot;(FOR TESTING PURPOSES: your runid is ${runid})&quot;
</code></pre><p>are mapped using</p>
<pre><code>  strings=step1.out
</code></pre><p>Here, prefix &#39;step1.&#39; says that &#39;out&#39; is coming from &#39;step1&#39;.</p>
<p>The example protocols has the following listings:</p>
<p>In our example variables &#39;date&#39; and &#39;wf&#39; are defined in 
an additional parameters file +workflow.defaults.csv+.</p>
<pre><code>  workflowName,creationDate
  myFirstWorkflow,today
</code></pre><p>In this way, the parameters can be divided in several groups and re-used in different workflows. If users do not like to map 
parameters, they should use the same names in protocols and parameters files. This makes parameters a kind of global.</p>
<h2>Generate workflow</h2><p>To generate actual workflow jobs, run the next command-line</p>
<pre><code>  sh molgenis_compute.sh --generate --parameters myfirst_workflow/parameters.csv --workflow myfirst_workflow/workflow.csv --defaults myfirst_workflow/workflow.defaults.csv
</code></pre><p>or with a short command-line version</p>
<pre><code>  sh molgenis_compute.sh -g -p myfirst_workflow/parameters.csv -w myfirst_workflow/workflow.csv -defaults myfirst_workflow/workflow.defaults.csv
</code></pre><p>The directory <code>rundir</code> is created.</p>
<pre><code>  ls rundir/
</code></pre><p>It contains a number of files</p>
<pre><code>  doc        step1_0.sh    step1_1.sh    step2_0.sh    submit.sh    user.env
</code></pre><p>.sh are actual scripts generated from the specified workflow. &#39;step1&#39; has two scripts and &#39;step2&#39; has only one, because it treats
outputs from scripts of the &#39;step1&#39; as a list, which is specified in step2.sh by</p>
<pre><code>    #list strings
</code></pre><p>user.env contains all actual parameters mappings. In this example:</p>
<pre><code>  #
  ## User parameters
  #
  creationDate[0]=&quot;today&quot;
  creationDate[1]=&quot;today&quot;
  input[0]=&quot;hello&quot;
  input[1]=&quot;bye&quot;
  workflowName[0]=&quot;myFirstWorkflow&quot;
  workflowName[1]=&quot;myFirstWorkflow&quot;
</code></pre><p>Parameters, which are known before hand can be connected to the environment file or weaved directly in the protocols (if &#39;weave&#39; flag is set in command-line options). In our example, two shell scripts are generated for 
the &#39;step1&#39;. The weaved version of generated files are shown below.</p>
<p>step1_0.sh:</p>
<pre><code>  #string in
  #output out
  # Let&#39;s do something with string &#39;in&#39;
  echo &quot;hello_hasBeenInStep1&quot;
  out=hello_hasBeenInStep1
</code></pre><p>and step1_1.sh</p>
<pre><code>  #string in
  #output out
  # Let&#39;s do something with string &#39;in&#39;
  echo &quot;bye_hasBeenInStep1&quot;
  out=bye_hasBeenInStep1
</code></pre><p>The output values of the first steps are not known beforehand, so, &#39;string&#39; cannot be weaved and will stay in the generated for the &#39;step2&#39; script as it was. However, the &#39;wf&#39; and &#39;date&#39; values are weaved.</p>
<p>step2_0.sh:</p>
<pre><code>  #string wf
  #string date
  #list strings
  echo &quot;Workflow name: myFirstWorkflow&quot;
  echo &quot;Created: today&quot;
  echo &quot;Result of step1.sh:&quot;
  for s in &quot;${strings[@]}&quot;
  do
      echo ${s}
  done
</code></pre><p>If values can be known, the script will have the following content </p>
<p>step2_0.sh with all known values:</p>
<pre><code>  #string wf
  #string date
  #list strings
  echo &quot;Workflow name: myFirstWorkflow&quot;
  echo &quot;Created: today&quot;
  echo &quot;Result of step1.sh:&quot;
  for s in &quot;hello&quot; &quot;bye&quot;
  do
      echo ${s}
  done
</code></pre><p>If &#39;weaved&#39; flag is not set, +step1_0.sh+ file, for example looks as follows:</p>
<pre><code>  # Connect parameters to environment
  input=&quot;bye&quot;
  #string input
  # Let&#39;s do something with string &#39;in&#39;
  echo &quot;${input}_hasBeenInStep1&quot;
  out=${input}_hasBeenInStep1
</code></pre><p>In this way, users can choose how generated files look like.
In the current implementation, values are first taken from parameter files. If they are not present, then compute looks,
if these values can be known at run-time, by analysing all previous steps of the protocol, where values are unknown.
If values cannot be known at run-time, compute will give a generation error. </p>
<h2>Execute workflow</h2><p>The workflow can be executed with the command</p>
<pre><code>  sh molgenis_compute.sh --run
  ls rundir/
</code></pre><p>Now, +rundir+ contains more files</p>
<pre><code>  doc                step1_0.sh.finished        step1_1.sh.finished        step2_0.sh.finished
  molgenis.bookkeeping.log    step1_0.sh.started        step1_1.sh.started        step2_0.sh.started
  step1_0.env            step1_1.env            step2_0.env            submit.sh
  step1_0.sh            step1_1.sh            step2_0.sh            user.env
</code></pre><p>.started and .finished files are created, when certain jobs are started and finished respectively. </p>
<p>In our example, &#39;strings&#39; variable from &#39;step2&#39; requires run-time values produced in &#39;step1&#39;. These values are taken from step1_X.env files. For example:</p>
<p>step1_0.env:</p>
<pre><code>  step1__has__out[0]=hello_hasBeenInStep1
</code></pre><p>In the workflow.csv file, it is specified with a simple &#39;.&#39; </p>
<p>  strings=step1.out</p>
<p>and substituted with &#39;<strong>has</strong>&#39; in generated script files.</p>
<h2>Command-line options</h2><p>Molgenis Compute has the following command-line options:</p>
<h3>MOLGENIS COMPUTE</h3><p>  Version: development
  usage: sh molgenis-compute.sh -p parameters.csv
  -b,--backend <arg>                 Backend for which you generate.
                                     Default: localhost
  -bp,--backendpassword <arg>        Supply user pass to login to execution
                                     backend. Default is not saved.
  -bu,--backenduser <arg>            Supply user name to login to execution
                                     backend. Default is your own user
                                     name.
  -clear,--clear                     Clear properties file
  -create <arg>                      Creates empty workflow. Default name:
                                     myworkflow
  -d,--database <arg>                Host, location of database. Default:
                                     none
  -dbe,--database-end                End the database
  -dbs,--database-start              Starts the database
  -defaults,--defaults <arg>         Path to your workflow-defaults file.
                                     Default: defaults.csv
  -footer <arg>                      Adds a custom footer. Default:
                                     footer.ftl
  -g,--generate                      Generate jobs
  -h,--help                          Shows this help.
  -header <arg>                      Adds a custom header. Default:
                                     header.ftl
  -l,--list                          List jobs, generated, queued, running,
                                     completed, failed
  -mpass,--molgenispassword <arg>    Supply user pass to login to molgenis.
                                     Default is not saved.
  -mu,--molgenisuser <arg>           Supply user name to login to molgenis
                                     database. Default is your own user
                                     name.
  -o,--overwrite <arg>               Parameters and values, which will
                                     overwritten in the parameters file.
                                     Parameters should be placed into
                                     brackets and listed using equality
                                     sign, e.g. &quot;mem=6GB;queue=long&quot;
  -p,--parameters <parameters .csv>   Path to parameter.csv file(s).
                                     Default: parameters.csv
  -path,--path                       Path to directory this generates to.
                                     Default: <current dir>.
  -port,--port <arg>                 Port used to connect to databasae.
                                     Default: 8080
  -r,--run                           Run jobs from current directory on
                                     current backend. When using --database
                                     this will return a &#39;id&#39; for --pilot.
  -rundir <arg>                      Directory where jobs are stored
  -runid,--runid <arg>               Id of the task set which you generate.
                                     Default: null
  -submit <arg>                      Set a custom submit.sh template.
                                     Default: submit.sh.ftl
  -w,--workflow <workflow .csv>       Path to your workflow file. Default:
                                     workflow.csv
  -weave,--weave                     Weave parameters to the actual script
  -web,--web <arg>                   Location of the workflow in the public
                                     github repository. The other
                                     parameters should be specified
                                     relatively to specified github root.</arg></workflow></arg></arg></arg></arg></current></parameters></arg></arg></arg></arg></arg></arg></arg></arg></arg></arg></arg></p>
<h2>Reserved words</h2><p>Molgenis Compute has a list of reserved words, which cannot be used in compute to name 
parameters, workflow steps, etc. These words are listed below:</p>
<p>  port            interval
  workflow        path
  defaults        parameters
  rundir        runid
  backend        database
  walltime        nodes
  ppn            queue
  mem            _NA
  password        molgenisuser
  backenduser        header
  footer        submit
  autoid</p>
<p>The reserved words are used in the compute.properties file. This file is created to save the latest
compute configuration and discuss further.</p>
<h2>Advanced parameter formats</h2><p>More parameters can be specified using the next format</p>
<p>  parameter1, parameter2
  value11,    value21
  value12,    value22
  value13,    value23</p>
<p>Alternatively, parameters can be specified in the +.properties+ style. The parameters file also should have
the +.properties+ extension.</p>
<p>  parameter1 = value11, value12, value13
  parameter2 = value21, value22, value23</p>
<p>Values for the workflow to iterate over can be passed as CSV file with parameter names in the header (a-zA-Z0-9 and underscore, starting with a-zA-Z) and parameter values in each row. Use quotes to escape commas, e.g. &quot;a,b&quot;.</p>
<p>Each value is one of the following:</p>
<p>. a string
. a Freemarker template [[X11]]<a href="http://freemarker.org/[Freemarker">http://freemarker.org/[Freemarker</a>]
. a series i..j (where i and j are two integers), meaning from i to j including i and j
. a list of &#39;;&#39; separated values (may contain templates)</p>
<p>You can combine multiple parameter files: the values will be &#39;natural joined&#39; based on overlapping columns.</p>
<p>Example with two or more parameter files:</p>
<p>  molgenis --path path/to/workflow -p f1.csv -p f2.csv -w workflow.csv</p>
<p>+f1.csv+ (white space will be trimmed):</p>
<p>  p0,  p2
  x,   1
  y,   2</p>
<p>+f2.csv+ (white space will be trimmed):</p>
<p>  p1,  p2,    p3,     p4
  v1,  1..2,  a;b,    file${p2}</p>
<p>Merged and expanded result for f1.csv + f2.csv:</p>
<p>  p0,  p1,  p2,   p3,   p4
  x,   v1,  1,    a,    file1 
  x,   v1,  1,    b,    file1
  y,   v1,  2,    a,    file2
  y,   v1,  2,    b,    file2</p>
<p>More complex parameter examples can combine values with template, as following:</p>
<p>  foo=    item1 , item2
  bar=    ${foo}, item3
  number= 123</p>
<p>Here, variable &#39;bar&#39; has two values of variable &#39;foo&#39;.</p>
<h3>Specifying workflow in parameters file</h3><p>Alternatively to specifying workflow in the command-line using &#39;-w&#39; or &#39;--workflow&#39;, workflow can be present as a parameter in 
+parameters.csv+ file:</p>
<p>  workflow, parameter1, parameter2
  workflow.csv, value1, value2</p>
<h3>Lists of parameters</h3><p>Parameters can be specified in several parameter files. To understand how &#39;list&#39; parameter specification works, let&#39;s consider the case with 2 parameter files and 1 protocol.</p>
<p>+parameters1.csv+</p>
<p>  project , sample
  project1, sample1
  project1, sample2
  project1, sample3</p>
<p>+parameters2.csv+</p>
<p>  chr
  chr1
  chr2
  chr3</p>
<p>The example protocol looks like</p>
<p>+protocol1.sh+</p>
<h1>!/bin/sh</h1><h1>string project</h1><h1>list sample</h1><h1>list chr</h1><p>  for s in &quot;${sample[@]}&quot;
  do
    echo $s
    for c in &quot;${chr[@]}&quot;
    do
         echo $c
    done
  done</p>
<p>Here, &#39;sample&#39; and &#39;chr&#39; parameters are coming from 2 different parameter files. Both parameters are specified as &#39;list&#39; in the protocol. These lists of parameters will not be combined, since they are coming from different parameters files.
The generated parameters lists will have the next look:</p>
<h1>!/bin/sh</h1><h1>string project</h1><h1>list sample</h1><h1>list chr</h1><p>  for s in &quot;sample1&quot; &quot;sample2&quot; &quot;sample3&quot;
  do
    echo $s
    for c in &quot;chr1&quot; &quot;chr2&quot; &quot;chr3&quot;
    do
         echo $c
    done
  done</p>
<p>If users want to combine lists that coming from separated files, lists should be declared on the same line, like</p>
<p>  list sample, chr</p>
<p>It will produce one list with all possible combination of parameters:</p>
<p>  sample1, chr1
  sample1, chr2
  sample1, chr3
  sample2, chr1
  sample2, chr2
  sample2, chr3
  sample3, chr1
  sample3, chr2
  sample3, chr3</p>
<p>It is not the desired behaviour in the considered protocol:</p>
<h1>!/bin/sh</h1><h1>string project</h1><h1>list sample, chr</h1><p>  for s in &quot;sample1&quot; &quot;sample1&quot; &quot;sample1&quot; &quot;sample2&quot; &quot;sample2&quot; &quot;sample2&quot; &quot;sample3&quot; &quot;sample3&quot; &quot;sample3&quot;
  do
    echo $s
    for c in &quot;chr1&quot; &quot;chr2&quot; &quot;chr3&quot; &quot;chr1&quot; &quot;chr2&quot; &quot;chr3&quot; &quot;chr1&quot; &quot;chr2&quot; &quot;chr3&quot;
    do
         echo $c
    done
  done</p>
<h2>Script generation for PBS cluster and other back-ends</h2><p>To generate for pbs, the next options should be added to the command line</p>
<p>--backend pbs</p>
<p>When generating script for computational clusters or grid, some additional parameters, such as execution wall-time, memory requirement, etc. should be 
specified.</p>
<p>This can be done in the parameters file</p>
<p>  workflowName,creationDate,queue,mem,walltime,nodes,ppn
  myFirstWorkflow,today,short_queue,4GB,05:59:00,1,1</p>
<p>  queue - cluster/grid queue
  mem - memory required
  walltime - execution wall time
  nodes - number of nodes needed
  ppn - number of cores needed</p>
<p>Or also it can be specified in the molgenis header in protocols</p>
<p>+step1.ftl with molgenis header+</p>
<h1>MOLGENIS queue=short_queue mem=4gb walltime=05:59:00 nodes=1 ppn=1</h1><h1>string in</h1><h1>output out</h1><h1>Let&#39;s do something with string &#39;in&#39;</h1><p>  out=${in}_hasBeenInStep1</p>
<p>The specification in protocols has priority over specification in parameter files.</p>
<p>In the command-line distribution, users can add a new back-end by adding a new directory, that contains header/footer and submit templates for that backend.</p>
<h2>Switching to a different workflow</h2><p>It is very advisable to start working with a new workflow with running </p>
<p>sh molgenis_compute.sh --clear</p>
<p>This command clears the +.compute.properties+ file, which contains previous generation and running options.</p>
<h2>Commenting in workflow specification</h2><p>User can want to run only one or several steps of the workflow, when the rest of workflow can be commented out using &#39;#&#39; sign. In this example
&#39;step2&#39; is commented out.</p>
<p>  step,protocol,dependencies
  step1,protocols/step1.sh,</p>
<h1>step2,protocols/step2.sh,step1</h1><h2>Database usage</h2><p>The detailed user gid for [[X12]]<a href="https://github.com/molgenis/molgenis-compute/tree/master/molgenis-app-compute-db[molgenis-compute">https://github.com/molgenis/molgenis-compute/tree/master/molgenis-app-compute-db[molgenis-compute</a> database version] will be added soon.
The application can be build as a maven project. During the first start of compute-db, the &#39;admin&#39; with password &#39;admin&#39; will be created. 
So, the users can login into the system for the first time. </p>
<p>To insert new compute runs into database, use the command-line like</p>
<p>  sh molgenis_compute.sh \
  --generate \
  --workflow myfirst_workflow/workflow.csv \
  --defaults myfirst_workflow/workflow.defaults.csv \
  --parameters myfirst_workflow/parameters.csv \
  --rundir run \
  --runid demo1 \
  --database <db-ip-address> \
  -mu <db-username> -mpass <db-password> \
  -b <back-end> \
  -bu <back-end-username> -bp <back-end-password></back-end-password></back-end-username></back-end></db-password></db-username></db-ip-address></p>
<p>After this your compute run will appear at the &#39;Jobs Dashboard&#39;. It will look like in the image below: 
User can submit jobs entering the user name and password for computational backend.</p>
<p>The jobs details can be expected in Tasks/History generated forms
Jobs also can be submitted and monitored from the command-line using &#39;--run&#39; flag. 
However, this functionality is not fully tested yet and submission via web-user interface is more stable.</p>
<h2>Available workflows</h2><p>The available (NGS alignment, imputation) workflows are available in the<br>[[X13]]<a href="https://github.com/molgenis/molgenis-pipelines[workflow">https://github.com/molgenis/molgenis-pipelines[workflow</a> github repository].</p><center><br><i>See something that could be better? <a href="https://github.com/molgenis/molgenis.org/tree/master/public/pipelines/mc-original.md">Edit this page at Github</a></i><br></center></div></div></div></div><footer class="molgenis-docs-footer"><p>MOLGENIS and these docs are maintained by the <a href="about/team">MOLGENIS team and collaborators</a>. <br/> Documentation source code released under the <a href="/LICENSE-MIT">MIT License</a>, documentation contents under <a href="/LICENSE-CC">CC BY 3.0</a>, and MOLGENIS software code under the <a href="/LICENSE-LGPLv3">LGPLv3 License</a>.<ul class="text-muted"><li>Version 1.8.0</li><li>·</li><li><a href="https://github.com/molgenis/molgenis">MOLGENIS Github</a></li><li>·</li><li><a href="http://github.com/molgenis/molgenis/issues">MOLGENIS Issues</a></li><li>·</li><li><a href="https://github.com/molgenis/molgenis.org/tree/master/public">Docs Github    </a></li><li>·</li><li><a href="http://github.com/molgenis/molgenis.org/issues">Docs Issues</a></li><li>·</li><li><a href="https://github.com/molgenis/molgenis/releases">Releases</a></li></ul></p></footer><script src="/res/js/prism.js"></script></body></html>